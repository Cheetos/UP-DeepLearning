{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e81be772",
   "metadata": {},
   "source": [
    "## RNN Sentiment\n",
    "\n",
    "A common dataset for language processing examples is the IMDB reviews dataset. It consists of 50000 movie reviews in English, along with a simple binary target from each review, indicating wheter it is negative (0) or positive (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af4be3",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, we will import libraries and define constants and functions that will help us during the examples in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12018c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "my_seed = 42\n",
    "np.random.seed(my_seed)\n",
    "tf.random.set_seed(my_seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"nlp\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccaa42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f56022",
   "metadata": {},
   "source": [
    "### Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774b849",
   "metadata": {},
   "source": [
    "The dataset is already preprocessed. `X_train` will contain a list of reviews, each represented an a NumPy array of integers, where each integer represents a word. All puntucations were removed, and the letters converted to lowercase, split by spaces, and finally, indexed by frequency. The integers 0, 1, and 2 represent the padding token, the *start-of-sequence* token, and unknown words, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f7f276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b0744",
   "metadata": {},
   "source": [
    "To know which are the first ten words of the first review of our training set we do the following. Notice that the first word is *\\<sos\\>*, which indicates the start of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb623a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358a67a",
   "metadata": {},
   "source": [
    "We proceed to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249bcd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27d5e3",
   "metadata": {},
   "source": [
    "Let's take a look to the first two reviews that we have, we are goingto print the first 200 characters of each review, as well their target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9647980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 16:55:54.852240: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-25 16:55:54.868847: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c234c0",
   "metadata": {},
   "source": [
    "Let's write a preprocessing function.\n",
    "\n",
    "* Truncate the reviews, keeping only the first 300 characters of each, in order to speed up training. We can generally tell wheter a review is positive or not in the first two sentences.\n",
    "* Replace \\<br /\\> tags with spaces.\n",
    "* Replace any character other than letters and quotes with spaces.\n",
    "* Split the reviews by the spaces\n",
    "* Convert the reviews to tensors, padding all reviews with the padding token, so they all have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d35918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 350)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd1efac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 61), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'piece', b'The', b'most', b'pathetic', b'scenes',\n",
       "         b'were', b'those', b'when', b'the', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Constantly', b'slow', b'and', b'boring', b'Things', b'seemed',\n",
       "         b'to', b'happen', b'b']], dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5845ab",
   "metadata": {},
   "source": [
    "Next, we need to construct the vocabulary. This requires going through the whole training set oncem applying our `preprocess` functionm and using `Counter` to count the number of ocurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4939773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e24d97",
   "metadata": {},
   "source": [
    "Let's look at the three most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a83d2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 245391), (b'the', 71886), (b'a', 44352)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ec2a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57722"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe71160",
   "metadata": {},
   "source": [
    "We may don't need our model to know all the words in the dictionary to get good performance, so let's truncate the vocabulary by keeping only the 10000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c394e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17188e53",
   "metadata": {},
   "source": [
    "Now, we need to add a preprocessing step to replace each word with its ID (i.e., its index in the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64ad462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "13\n",
      "12\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "for word in b\"This movie was faaaaaantastic\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee98e6e",
   "metadata": {},
   "source": [
    "To implement this preprocessing step, we will create a lookup table using 1000 out-of-vocabulary buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4102985",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3721b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   24,    13,    12, 20053]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be08ac9",
   "metadata": {},
   "source": [
    "Now, we proceed to create the final training set. We bath the reviews, convert them to short sentences if words with the `preprocess` function, then encode these words using the lookup table. Finally, prefetcg the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee6d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5fb720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  24   12   28 ...    0    0    0]\n",
      " [   7   21   72 ...    0    0    0]\n",
      " [3965 6829    1 ...    0    0    0]\n",
      " ...\n",
      " [  24   13  121 ...    0    0    0]\n",
      " [1899 4215  465 ...    0    0    0]\n",
      " [3589 4756    7 ...    0    0    0]], shape=(32, 70), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45c73f",
   "metadata": {},
   "source": [
    "Finally, let's create the model and train it.\n",
    "\n",
    "The first layer is an `Embedding` layer, which converts words IDs into embeddings. The embedding matrix needs to have one row per word ID, and one column per embedding dimension, which for this example is 128. The output of the `Embedding` layer will be 3D *[batch size, time steps, embedding size]*.\n",
    "\n",
    "The rest of the model is composed by two GRU layers, with the second ine returning only the output of the last time step. The output layer consists of a single neuron with the sigmoid activation function to estimate the sentiment with a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "901e6a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 68s 83ms/step - loss: 0.5102 - accuracy: 0.7427\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 65s 83ms/step - loss: 0.2871 - accuracy: 0.8868\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 66s 85ms/step - loss: 0.1305 - accuracy: 0.9540\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.0908 - accuracy: 0.9665\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.0522 - accuracy: 0.9810\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf16d283",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9427bb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw TH ...\n",
      "Label: 1 = Positive\n",
      "\n",
      "Review: A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses ...\n",
      "Label: 1 = Positive\n",
      "\n",
      "Review: Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. Now I'll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you've milked a franchise so bad when you can see t ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: Poor Shirley MacLaine tries hard to lend some gravitas to this mawkish, gag-inducing \"feel-good\" movie, but she's trampled by the run-away sentimentality of a film that's not the least bit grounded in reality.<br /><br />This was directed by Curtis Hanson? Did he have a lobotomy since we last heard from him? Hanson can do effective drama sprinkled  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: As a former Erasmus student I enjoyed this film very much. It was so realistic and funny. It really picked up the spirit that exists among Erasmus students. I hope, many other students will follow this experience, too. However, I wonder if this movie is all that interesting to watch for people with no international experience. But at least one of m ...\n",
      "Label: 1 = Positive\n",
      "\n",
      "Review: My God, Ryan Gosling has made a lot of deep characters in his career, this is one of his wonderful acting jobs. For me this is a very deep movie, needs a lot of concentration, not because is difficult to watch, just because you understand it if you put your shoes in this kid, even though has everything and has famous father that is a writer, has a  ...\n",
      "Label: 1 = Positive\n",
      "\n",
      "Review: This film just won the best film award at the Cleveland International Film Festival. It's American title apparently is Autumn Spring. The acting is superb. The story takes you into the life of an elderly man who takes what life deals him and spikes it up a little bit. Abetted by his best friend (and partner in not-so-serious crime) he puts people o ...\n",
      "Label: 1 = Positive\n",
      "\n",
      "Review: The cast for this production of Rigoletto is excellent. Edita Gruberova sings Gilda magnificently and passionately. Luciano Pavarotti also sings splendidly. Vergara is a fine Maddalena; Fedora Barbieri is a famous older singer who sings the maid, Giovanna. Weikl sings Marullo; Wixell sings both Rigoletto and Monterone. As Rigoletto, Wixell is proba ...\n",
      "Label: 1 = Positive\n",
      "\n",
      "Review: As long as you keep in mind that the production of this movie was a copyright ploy, and not intended as a serious release, it is actually surprising how not absolutely horrible it is. I even liked the theme music.<br /><br />And if ever a flick cried out for a treatment by Joel (or Mike) and the MST3K Bots, this is it! Watch this with a bunch of sm ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: Every great once in a while, you stumble upon a movie that exceeds even your wildest expectations. Given the IMDb rating of 4.0, I wasn't really expecting much with The Brotherhood of Satan. I hoped that at a minimum it might be cheesy fun like The Devil's Rain or any of the other early 70s similarly themed Satanic horror films. I couldn't' have be ...\n",
      "Label: 1 = Positive\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:01:34.692891: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for X_test_batch, y_test_batch in datasets[\"test\"].batch(10).take(1):\n",
    "    for review, label in zip(X_test_batch.numpy(), y_test_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:350], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98ce5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_set = datasets[\"test\"].batch(10).take(1)\n",
    "my_test_set = my_test_set.map(preprocess)\n",
    "my_test_set = my_test_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cf51873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 930ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:01:35.575952: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(my_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f627803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 17:01:35.717541: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1 - Positive',\n",
       " '0 - Negative',\n",
       " '0 - Negative',\n",
       " '0 - Negative',\n",
       " '0 - Negative',\n",
       " '1 - Positive',\n",
       " '1 - Positive',\n",
       " '1 - Positive',\n",
       " '0 - Negative',\n",
       " '0 - Negative']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"1 - Positive\" if pred > 0.5 else \"0 - Negative\" for pred in model.predict(my_test_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91205129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.7829018e-01],\n",
       "       [1.8437991e-03],\n",
       "       [5.6936301e-04],\n",
       "       [2.2718550e-03],\n",
       "       [2.1810536e-01],\n",
       "       [9.9974781e-01],\n",
       "       [9.9989420e-01],\n",
       "       [9.9982154e-01],\n",
       "       [2.4527591e-04],\n",
       "       [1.3223001e-03]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9bb68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
