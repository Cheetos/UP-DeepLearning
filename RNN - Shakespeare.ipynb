{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e81be772",
   "metadata": {},
   "source": [
    "## RNN Shakespeare\n",
    "\n",
    "The goal is to train a model that can predict the next time step (four notes), given a sequence of time steps from a chorale. Then use this model to generate Bach-like music, one note at a time; this can be accomplished by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af4be3",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, we will import libraries and define constants and functions that will help us during the examples in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12018c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "my_seed = 42\n",
    "np.random.seed(my_seed)\n",
    "tf.random.set_seed(my_seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"nlp\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22cc48",
   "metadata": {},
   "source": [
    "To better understand how tensorflow process data, we will process the sequence 0 to 14 in the following way.\n",
    "\n",
    "* Process each element of the sequence individually: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "* split the data into windows of length 5 with a shift of 2. \n",
    "\n",
    "       [0, 1, 2, 3, 4]\n",
    "\n",
    "       [2, 3, 4, 5, 6]\n",
    "\n",
    "       [4, 5, 6, 7, 8]\n",
    "\n",
    "       [6, 7, 8, 9, 10]\n",
    "\n",
    "       [8, 9, 10, 11, 12]\n",
    "\n",
    "       [10, 11, 12, 13, 14]\n",
    "\n",
    "* Create a 1D sequence of windows: [[0, 1, 2, 3, 4], [2, 3, 4, 5, 6], [4, 5, 6, 7, 8], [6, 7, 8, 9, 10], [8, 9, 10, 11, 12], [10, 11, 12, 13, 14]]\n",
    "\n",
    "* Shuffle the elements of the sequence and separate the target variable. If we don't shuffle we would see something like this:\n",
    "\n",
    "       [([0, 1, 2, 3], [1, 2, 3, 4]),\n",
    "    \n",
    "        ([2, 3, 4, 5],  [3, 4, 5, 6]),\n",
    "    \n",
    "        ([4, 5, 6, 7],  [5, 6, 7, 8]),\n",
    "    \n",
    "        ([6, 7, 8, 9],  [7, 8, 9, 10]),\n",
    "    \n",
    "        ([8, 9, 10, 11],[9, 10, 11, 12]),\n",
    "    \n",
    "        ([10, 11, 12, 13],[11, 12, 13, 14])]\n",
    "    \n",
    "* Finally, create batches of size 3\n",
    "\n",
    "#### Batch 1\n",
    "\n",
    "    [([0, 1, 2, 3], [1, 2, 3, 4]),\n",
    "    \n",
    "    ([2, 3, 4, 5],  [3, 4, 5, 6]),\n",
    "    \n",
    "    ([4, 5, 6, 7],  [5, 6, 7, 8]),\n",
    "    \n",
    "#### Batch 2\n",
    "    \n",
    "    ([6, 7, 8, 9],  [7, 8, 9, 10]),\n",
    "    \n",
    "    ([8, 9, 10, 11],[9, 10, 11, 12]),\n",
    "    \n",
    "    ([10, 11, 12, 13],[11, 12, 13, 14])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccaa42bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[6 7 8 9]\n",
      " [2 3 4 5]\n",
      " [4 5 6 7]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 7  8  9 10]\n",
      " [ 3  4  5  6]\n",
      " [ 5  6  7  8]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[ 0  1  2  3]\n",
      " [ 8  9 10 11]\n",
      " [10 11 12 13]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 1  2  3  4]\n",
      " [ 9 10 11 12]\n",
      " [11 12 13 14]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 12:43:39.185619: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" * 5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f56022",
   "metadata": {},
   "source": [
    "### Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774b849",
   "metadata": {},
   "source": [
    "First, we download all Shakespare work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f7f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb623a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358a67a",
   "metadata": {},
   "source": [
    "We can get the distinct characters used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249bcd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\n",
      "Number of chars:  39\n"
     ]
    }
   ],
   "source": [
    "char_set = \"\".join(sorted(set(shakespeare_text.lower())))\n",
    "print(char_set)\n",
    "print(\"Number of chars: \", len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65630fe0",
   "metadata": {},
   "source": [
    "The Keras's Tokenizer class encodes every character as an integer, it wuill find all all the characters used and map each of them to a different ID, from 1 to the number of distinct characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d329e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887d5f4",
   "metadata": {},
   "source": [
    "We use `char_level=True` to get character-level encoding, rather than the default word-level encoding. The tokenizer converts all the text to lowercase by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e3ad86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 7, 2, 2, 3, 4]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"Cheeto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e1577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c h e e t o']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[19, 7, 2, 2, 3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27d5e3",
   "metadata": {},
   "source": [
    "From the tokenizer, we can get the distinct chatacters used, as well as the total number of characters used. Note that the number of different characters matches with the previous result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9647980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_id =  39\n",
      "dataset_size =  1115394\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters\n",
    "\n",
    "print(\"max_id = \", max_id)\n",
    "print(\"dataset_size = \", dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c234c0",
   "metadata": {},
   "source": [
    "Now that we have the tokenizer, we proceed to encode the whole text with numbers from 0 to 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d35918",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52ecf6",
   "metadata": {},
   "source": [
    "Then we get the training set. It is important to avoid overlap between the training, validation, and test set. When dealing with time series, you would in general split across time; for example from a start date to an end date. It is ofeten safer to split across time, bit this assumes that the patterns the RNN can learn in the past, will still exist in the future.\n",
    "\n",
    "In short, splitting a time series into a training set, validation set, and test set is not a trivial task. \n",
    "\n",
    "For this example we will take the first 90% of the text for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd1efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5845ab",
   "metadata": {},
   "source": [
    "Currently, the training set is a single sequence of over a million characters, so we can't just use it to feed the RNN; so we will split the data into windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4939773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e24d97",
   "metadata": {},
   "source": [
    "By default, the `window` method creates nonoverlapping windows, but to get the largest possible training set, we use `shift=1`. So the first window have the characters 0 to 100, the second window contains characters 1 to 101, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a83d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338e16f",
   "metadata": {},
   "source": [
    "The `window` method creates a nested dataset, but we can't use this for training, since the model expects tensors, so we use the `flat_map` method, that converts the nested dataset into a flat dataset, and by using the `badge` method, we get a flat dataset of tensors of size `window_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ec2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(my_seed)\n",
    "tf.random.set_seed(my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe71160",
   "metadata": {},
   "source": [
    "Now, we shuffle the windows and batch the windows. Then, we can separate the targets from the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c394e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17188e53",
   "metadata": {},
   "source": [
    "When training a model, categorical features should be encoded, ususally using the one-hot encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64ad462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee98e6e",
   "metadata": {},
   "source": [
    "Finally, we just need to add prefetching, that will do its best to always be one batch ahead, so while the training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4102985",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3721b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be08ac9",
   "metadata": {},
   "source": [
    "To predict the next character based on the previous 100 characters, we can use an RNN with 2 GRU layers of 128 units each, and a 20% dropout on both the inputs and hidden states (`recurrent_dropout`).\n",
    "\n",
    "The last Dense layer must have 29 units, because there are 39 distinct characters, and we want to output a probability for each possible character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eee6d553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31368/31368 [==============================] - 4570s 146ms/step - loss: 1.6197\n",
      "Epoch 2/10\n",
      "31368/31368 [==============================] - 4197s 134ms/step - loss: 1.5395\n",
      "Epoch 3/10\n",
      "31368/31368 [==============================] - 4953s 158ms/step - loss: 1.5196\n",
      "Epoch 4/10\n",
      "31368/31368 [==============================] - 4301s 137ms/step - loss: 1.5086\n",
      "Epoch 5/10\n",
      "31368/31368 [==============================] - 4268s 136ms/step - loss: 1.5020\n",
      "Epoch 6/10\n",
      "31368/31368 [==============================] - 4826s 154ms/step - loss: 1.4967\n",
      "Epoch 7/10\n",
      "31368/31368 [==============================] - 5250s 167ms/step - loss: 1.4931\n",
      "Epoch 8/10\n",
      "31368/31368 [==============================] - 4544s 145ms/step - loss: 1.4902\n",
      "Epoch 9/10\n",
      "31368/31368 [==============================] - 4641s 148ms/step - loss: 1.4880\n",
      "Epoch 10/10\n",
      "31368/31368 [==============================] - 4638s 148ms/step - loss: 1.4854\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40744e8",
   "metadata": {},
   "source": [
    "The `preprocess` function uses the tokenizer and the one-hot encoder in some input to be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5fb720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45c73f",
   "metadata": {},
   "source": [
    "Let's try a small example consisting on predicting the next character in a string. The result is the character with highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "901e6a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = np.argmax(model(X_new), axis=-1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bd57c",
   "metadata": {},
   "source": [
    "Now, let's use the `tf.random.categorical` method to sample the next character according to their probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91481199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 1, 0, 2, 1,\n",
       "        0, 1, 2, 1, 1, 1, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(my_seed)\n",
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c5a39b",
   "metadata": {},
   "source": [
    "As the `temperature` parameter increses, the probabilities are smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "324f39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9390e21",
   "metadata": {},
   "source": [
    "The `complete_text` method generates the next 50 characters from a given text; this is done by predicting the next character 50 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49a88c53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=100, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "067f2240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t the maid of me;\n",
      "the more that she may contributors and so good for me as free\n",
      "for the maid and be t\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(my_seed)\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1c4db69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ti no music, i putt you with\n",
      "these mettle\n",
      "shom his being unto the chrsem ypors\n",
      "and seek\n",
      "to mine own c\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9e41b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellow the men and see\n",
      "that she may contrace to her father is a fitther for my hands.\n",
      "\n",
      "gremio:\n",
      "no, sir, i\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"hello\", temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faac107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what i will not have the head?\n",
      "\n",
      "petruchio:\n",
      "sir, i shall not be so so stand the men and see the state\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"what\", temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5eb948fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hio:\n",
      "we would; or my mind hands; my father ever in my pethel? or to be not rap enough,\n",
      "ded you at his \n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"hi\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b1da9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do:\n",
      "her father comes your elders lucentio.\n",
      "\n",
      "duke vincentio:\n",
      "sir, gold the swelt and naw peruses a prov\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576fba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
